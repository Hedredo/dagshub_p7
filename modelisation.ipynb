{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import dagshub\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "import warnings\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"hedredo/dagshub_p7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"hedredo/dagshub_p7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository hedredo/dagshub_p7 initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository hedredo/dagshub_p7 initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 314\n"
     ]
    }
   ],
   "source": [
    "# Remove FutureWarning alerts\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Initialize dagshub repo\n",
    "try:\n",
    "    dagshub.init(repo_owner=\"hedredo\", repo_name=\"dagshub_p7\", mlflow=True)\n",
    "    mlflow.set_experiment(\"p7\")\n",
    "except Exception as e:\n",
    "    print(\"Dagshub repo can't be initialized:\", e)\n",
    "\n",
    "# Initialiser tqdm pour pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Set a random seed\n",
    "SEED = 314\n",
    "np.random.seed(SEED)\n",
    "print(\"Random seed set to\", SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow framework: GPU is available\n",
      "Pytorch framework: CUDA is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU and CUDA are available\n",
    "gpu = tf.config.list_physical_devices(\"GPU\")\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"Tensorflow framework: GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")\n",
    "print(\"Pytorch framework: CUDA is\", \"available\" if cuda else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chargement des données préparées**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMMENTS**:\n",
    "- Chargement des données du parquet en entier\n",
    "- Ou supprimer cette section et passer le chargement du parquet dans séparation des données - split data\n",
    "- Import de token_params pour les paramètres de tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickle file containing the columns\n",
    "with open(\"./data/processed/columns.pkl\", \"rb\") as f:\n",
    "    cols = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/processed/df_preprocessed.parquet\"\n",
    "df = pd.read_parquet(\n",
    "    path,\n",
    "    columns=[\"text\", \"target\"],\n",
    "    engine=\"pyarrow\",\n",
    "    use_nullable_dtypes=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Séparation des données**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMMENTS**:\n",
    "- Charger le parquet dans la fonction si possible en fonction de la liste token_params\n",
    "- Mettre un argument pour la liste des colonnes à charger sinon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, test_split=0.2, sampling=True, proportion=0.01):\n",
    "    \"\"\"\n",
    "    Split the data into train and test sets\n",
    "    :param df: Dataframe to split\n",
    "    :param n_rows: Number of rows in the dataframe\n",
    "    :return: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the data with sampling or\n",
    "    if sampling:\n",
    "        df = df.sample(frac=proportion, random_state=SEED)\n",
    "\n",
    "    # Define X and y\n",
    "    X, y = df.iloc[:, 0], df[\"target\"]\n",
    "\n",
    "    # Split the data with a 0.2 test size\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_split, stratify=y, random_state=SEED\n",
    "    )\n",
    "\n",
    "    # Display shape of splits\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "    # Return the splits\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (12772,)\n",
      "X_test shape: (3194,)\n",
      "y_train shape: (12772,)\n",
      "y_test shape: (3194,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(\n",
    "    df, test_split=0.2, sampling=True, proportion=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_align(col, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test):\n",
    "    # Load the df with the column name\n",
    "    df = pd.read_parquet(\n",
    "        path,\n",
    "        columns=[col, \"target\"],\n",
    "        engine=\"pyarrow\",\n",
    "        use_nullable_dtypes=False,\n",
    "    )\n",
    "    # Get the indexes to keep the same order after alignment\n",
    "    train_index = X_train.index\n",
    "    test_index = X_test.index\n",
    "\n",
    "    # Split the data with alignment and reindexing to keep the same order\n",
    "    X_train, _ = df.filter(like=col).align(X_train, join=\"inner\", axis=0)\n",
    "    X_train = X_train.reindex(train_index)\n",
    "    X_test, _ = df.filter(like=col).align(X_test, join=\"inner\", axis=0)\n",
    "    X_test = X_test.reindex(test_index)\n",
    "    y_train, _ = df.target.align(y_train, join=\"inner\", axis=0)\n",
    "    y_train = y_train.reindex(train_index)\n",
    "    y_test, _ = df.target.align(y_test, join=\"inner\", axis=0)\n",
    "    y_test = y_test.reindex(test_index)\n",
    "\n",
    "    # Return the aligned and sorted new splits\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modélisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMMENTS**:\n",
    "- Création du pipeline modulable\n",
    "- Grille de paramètres pour le vectorizer\n",
    "- Grille de paramètres pour les modèles (LG, MNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = False\n",
    "if experiment:\n",
    "    with mlflow.start_run():\n",
    "        # Load the file\n",
    "        # Fit and transform the tf-idf vectorizer on the text column\n",
    "        tfidf = TfidfVectorizer(\n",
    "            ngram_range=(1, 3),\n",
    "            max_features=1000,\n",
    "            strip_accents=\"unicode\",\n",
    "        )\n",
    "        X_embed = tfidf.fit_transform(X_train[\"text\"].str.lower())\n",
    "        # Initialize the model\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        # Add the input example\n",
    "        input_logit = X_embed[0]\n",
    "        input_tfidf = X_train[\"text\"].str.lower().values[0]\n",
    "        # Perform a cross-validation\n",
    "        scores = cross_validate(\n",
    "            model, X_embed, y_train, cv=5, scoring=[\"accuracy\", \"f1\"]\n",
    "        )\n",
    "        # Your training code here...\n",
    "        model.fit(X_embed, y_train)\n",
    "        y_pred = model.predict(tfidf.transform(X_test[\"text\"].str.lower()))\n",
    "        scores = pd.DataFrame(scores).mean()\n",
    "        acc_score = accuracy_score(y_test, y_pred)\n",
    "        for metric in scores.keys():\n",
    "            mlflow.log_metric(f\"val_{metric}\", scores[metric])\n",
    "        mlflow.log_metric(\"test_accuracy\", acc_score)\n",
    "        mlflow.log_param(\"Dimension\", X_embed.shape[1])\n",
    "        mlflow.log_param(\"Tf-Idf params\", tfidf.get_params())\n",
    "        mlflow.log_param(\"Logistic Regression params\", model.get_params())\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \"logistic_regression\", input_example=input_logit\n",
    "        )\n",
    "        mlflow.sklearn.log_model(\n",
    "            tfidf, \"tfidf_vectorizer\", input_example=input_tfidf, signature=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the tf-idf vectorizer on the text column\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),\n",
    "    max_features=1000,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "X_embed = tfidf.fit_transform(X_train[\"text\"].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time         0.003252\n",
       "score_time       0.002088\n",
       "test_accuracy    0.722909\n",
       "test_f1          0.718497\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Perform a cross-validation\n",
    "scores = cross_validate(model, X_embed, y_train, cv=5, scoring=[\"accuracy\", \"f1\"])\n",
    "\n",
    "# Display the scores\n",
    "pd.DataFrame(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time         0.023068\n",
       "score_time       0.002958\n",
       "test_accuracy    0.735358\n",
       "test_f1          0.735984\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Perform a cross-validation\n",
    "scores = cross_validate(model, X_embed, y_train, cv=5, scoring=[\"accuracy\", \"f1\"])\n",
    "\n",
    "# Display the scores\n",
    "pd.DataFrame(scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous avons énormément de données à analyser, nous allons tester la performance du modèle en utilisant uniquement les features à notre disposition.<br>\n",
    "Le modèle sera plus rapide s'il reçoit en entrée une sparse matrix plutôt qu'une dense matrix.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Options de standardisation des textes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seed = SEED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text datagenerator for the training set\n",
    "train_datagen = TextVectorization(\n",
    "    max_tokens=1000,\n",
    "    output_sequence_length=100,\n",
    "    output_mode=\"int\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/engine/base_preprocessing_layer.py\", line 123, in adapt_step  *\n        self.update_state(data)\n    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 470, in update_state  **\n        self._lookup_layer.update_state(self._preprocess(data))\n    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 555, in _preprocess\n        inputs = self._standardize(inputs)\n    File \"/tmp/ipykernel_41092/358720669.py\", line 8, in spacy_preprocess\n        doc = nlp(text)\n    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/spacy/language.py\", line 1040, in __call__\n        doc = self._ensure_doc(text)\n    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/spacy/language.py\", line 1134, in _ensure_doc\n        raise ValueError(Errors.E1041.format(type=type(doc_like)))\n\n    ValueError: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'tensorflow.python.framework.ops.Tensor'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[95], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the datagenerator on the training set\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_datagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py:467\u001b[0m, in \u001b[0;36mTextVectorization.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n",
      "\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madapt\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;32m    418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes a vocabulary of string terms from tokens in a dataset.\u001b[39;00m\n",
      "\u001b[1;32m    419\u001b[0m \n",
      "\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    Calling `adapt()` on a `TextVectorization` layer is an alternative to\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m          argument is not supported with array inputs.\u001b[39;00m\n",
      "\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/engine/base_preprocessing_layer.py:258\u001b[0m, in \u001b[0;36mPreprocessingLayer.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n",
      "\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n",
      "\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n",
      "\u001b[0;32m--> 258\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_adapt_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    259\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n",
      "\u001b[1;32m    260\u001b[0m             context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filermn3q_ef.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__adapt_step\u001b[0;34m(iterator)\u001b[0m\n",
      "\u001b[1;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mnext\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(iterator),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;32m     10\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_adapt_maybe_build, (ag__\u001b[38;5;241m.\u001b[39mld(data),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;32m---> 11\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py:470\u001b[0m, in \u001b[0;36mTextVectorization.update_state\u001b[0;34m(self, data)\u001b[0m\n",
      "\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n",
      "\u001b[0;32m--> 470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lookup_layer\u001b[38;5;241m.\u001b[39mupdate_state(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py:555\u001b[0m, in \u001b[0;36mTextVectorization._preprocess\u001b[0;34m(self, inputs)\u001b[0m\n",
      "\u001b[1;32m    553\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mstrings\u001b[38;5;241m.\u001b[39mregex_replace(inputs, DEFAULT_STRIP_REGEX, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_standardize):\n",
      "\u001b[0;32m--> 555\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    558\u001b[0m     \u001b[38;5;66;03m# If we are splitting, we validate that the 1st axis is of dimension\u001b[39;00m\n",
      "\u001b[1;32m    559\u001b[0m     \u001b[38;5;66;03m# 1 and so can be squeezed out. We do this here instead of after\u001b[39;00m\n",
      "\u001b[1;32m    560\u001b[0m     \u001b[38;5;66;03m# splitting for performance reasons - it's more expensive to squeeze\u001b[39;00m\n",
      "\u001b[1;32m    561\u001b[0m     \u001b[38;5;66;03m# a ragged tensor.\u001b[39;00m\n",
      "\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\n",
      "Cell \u001b[0;32mIn[93], line 8\u001b[0m, in \u001b[0;36mspacy_preprocess\u001b[0;34m(text)\u001b[0m\n",
      "\u001b[1;32m      6\u001b[0m as_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Process the text\u001b[39;00m\n",
      "\u001b[0;32m----> 8\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m params:\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/spacy/language.py:1040\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n",
      "\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n",
      "\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[1;32m   1021\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1024\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[1;32m   1025\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n",
      "\u001b[1;32m   1026\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n",
      "\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n",
      "\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1038\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n",
      "\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m-> 1040\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m   1042\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/spacy/language.py:1134\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n",
      "\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n",
      "\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "\u001b[0;32m-> 1134\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE1041\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(doc_like)))\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n",
      "\n",
      "    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/engine/base_preprocessing_layer.py\", line 123, in adapt_step  *\n",
      "        self.update_state(data)\n",
      "    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 470, in update_state  **\n",
      "        self._lookup_layer.update_state(self._preprocess(data))\n",
      "    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 555, in _preprocess\n",
      "        inputs = self._standardize(inputs)\n",
      "    File \"/tmp/ipykernel_41092/358720669.py\", line 8, in spacy_preprocess\n",
      "        doc = nlp(text)\n",
      "    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/spacy/language.py\", line 1040, in __call__\n",
      "        doc = self._ensure_doc(text)\n",
      "    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/spacy/language.py\", line 1134, in _ensure_doc\n",
      "        raise ValueError(Errors.E1041.format(type=type(doc_like)))\n",
      "\n",
      "    ValueError: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Fit the datagenerator on the training set\n",
    "train_datagen.adapt(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text datagenerator for the training set\n",
    "train_datagen = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=1000,\n",
    "    output_sequence_length=100,\n",
    "    output_mode=\"int\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 18:48:01.481859: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Fit the datagenerator on the training set\n",
    "train_datagen.adapt(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12772, 100), dtype=int64, numpy=\n",
       "array([[329,  25,  87, ...,   0,   0,   0],\n",
       "       [449,   1, 468, ...,   0,   0,   0],\n",
       "       [  1,  62,  19, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [177,  15,   5, ...,   0,   0,   0],\n",
       "       [ 58,   8, 698, ...,   0,   0,   0],\n",
       "       [270,   1,   4, ...,   0,   0,   0]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at the output of the datagenerator\n",
    "train_datagen(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text datagenerator for the training set\n",
    "train_datagen = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=1000,\n",
    "    output_sequence_length=100,\n",
    "    output_mode=\"int\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 13:28:24.761537: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Fit the datagenerator on the training set\n",
    "train_datagen.adapt(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12772, 100), dtype=int64, numpy=\n",
       "array([[337,  25,   1, ...,   0,   0,   0],\n",
       "       [  1,   1,   1, ...,   0,   0,   0],\n",
       "       [  1,  52,  16, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [176,  14,   5, ...,   0,   0,   0],\n",
       "       [ 50,   8, 699, ...,   0,   0,   0],\n",
       "       [361,   1,   4, ...,   0,   0,   0]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at the output of the datagenerator\n",
    "train_datagen(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "max_features = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 16)          16000     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, None, 16)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,017\n",
      "Trainable params: 16,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Embedding(max_features, embedding_dim),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.losses.BinaryCrossentropy(),\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile a train dataset with X_train and y_train to pass it in model.fit\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train[\"text\"].values, y_train.values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name='embedding_2_input'), name='embedding_2_input', description=\"created by layer 'embedding_2_input'\"), but it was called on an input with incompatible shape ().\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 232, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_1\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"global_average_pooling1d_1\" is incompatible with the layer: expected ndim=3, found ndim=1. Full shape received: (16,)\n    \n    Call arguments received by layer \"sequential_1\" \"                 f\"(type Sequential):\n      • inputs=tf.Tensor(shape=(), dtype=string)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[123], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# validation_data=val_ds,\u001b[39;49;00m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n",
      "\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n",
      "\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileji1t0y2m.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n",
      "\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n",
      "\n",
      "    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/engine/training.py\", line 993, in train_step\n",
      "        y_pred = self(x, training=True)\n",
      "    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/home/hedredo/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 232, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Exception encountered when calling layer \"sequential_1\" \"                 f\"(type Sequential).\n",
      "    \n",
      "    Input 0 of layer \"global_average_pooling1d_1\" is incompatible with the layer: expected ndim=3, found ndim=1. Full shape received: (16,)\n",
      "    \n",
      "    Call arguments received by layer \"sequential_1\" \"                 f\"(type Sequential):\n",
      "      • inputs=tf.Tensor(shape=(), dtype=string)\n",
      "      • training=True\n",
      "      • mask=None\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    # validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
