{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:36:18.623416: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-09 17:36:19.281102: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdirectml.d6f03b303ac3c4f2eeb8ca631688c9757b361310.so\n",
      "2024-11-09 17:36:19.281154: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdxcore.so\n",
      "2024-11-09 17:36:19.287287: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libd3d12.so\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "2024-11-09 17:36:19.418654: I tensorflow/c/logging.cc:34] DirectML device enumeration: found 1 compatible adapters.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfasttext\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import mlflow.sklearn\n",
    "import warnings\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from gensim.models import KeyedVectors\n",
    "from huggingface_hub import hf_hub_download\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 314\n"
     ]
    }
   ],
   "source": [
    "# Remove FutureWarning alerts\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Initialiser tqdm pour pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Set a random seed\n",
    "SEED = 314\n",
    "np.random.seed(SEED)\n",
    "print(\"Random seed set to\", SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow framework: GPU is available\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU and CUDA are available\n",
    "gpu = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"Tensorflow framework: GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMMENTS**:\n",
    "- Chargement des données du parquet en entier\n",
    "- Ou supprimer cette section et passer le chargement du parquet dans séparation des données - split data\n",
    "- Import de token_params pour les paramètres de tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickle file containing the columns\n",
    "with open(\"../data/processed/columns.pkl\", \"rb\") as f:\n",
    "    cols = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the columns in cols moving the column after hour column\n",
    "cols = cols.reindex([\"hour\", \"target\", \"text\", *cols[3:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['hour', 'target', 'text', 'tokenizer with lowercase',\n",
       "        'tokenizer with lowercase, handle stripping, and length reduction',\n",
       "        'tokenizer with lowercase and alpha',\n",
       "        'tokenizer with lowercase, alpha and emoji',\n",
       "        'tokenizer with lowercase, alpha, and no stop words',\n",
       "        'tokenizer with lowercase, alpha and emoji, and no stop words'],\n",
       "       dtype='object'),\n",
       " array([2, 0, 1, 3, 4, 5, 6, 7, 8]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/processed/df_preprocessed.parquet\"\n",
    "df = pd.read_parquet(\n",
    "    PATH,\n",
    "    columns=[\"text\", \"target\"],\n",
    "    engine=\"pyarrow\",\n",
    "    use_nullable_dtypes=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1596630 entries, 0 to 799999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   text    1596630 non-null  object\n",
      " 1   target  1596630 non-null  int8  \n",
      "dtypes: int8(1), object(1)\n",
      "memory usage: 25.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Séparation des données**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMMENTS**:\n",
    "- Charger le parquet dans la fonction si possible en fonction de la liste token_params\n",
    "- Mettre un argument pour la liste des colonnes à charger sinon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, test_split=0.2, sampling=True, proportion=0.01, stratify=True):\n",
    "    \"\"\"\n",
    "    Split the data into train and test sets\n",
    "    :param test_split: Proportion of the data to include in the test split\n",
    "    :param sampling: Whether to sample the data\n",
    "    :param proportion: Proportion of the data to sample\n",
    "    :return: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Sample the data if needed to reduce the size\n",
    "    if sampling:\n",
    "        df_sample = df.sample(frac=proportion, random_state=SEED)\n",
    "    else:\n",
    "        df_sample = df\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_sample.iloc[:, 0],\n",
    "        df_sample.iloc[:, -1],\n",
    "        test_size=test_split,\n",
    "        stratify=df_sample.iloc[:, -1] if stratify else None,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    # Delete the sample dataframe to free up memory\n",
    "    del df_sample\n",
    "    # Return the train and test sets\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_splits_from_parquet(X_train, X_test, y_train, y_test, col_name, path=PATH):\n",
    "    # Read the parquet file with corpus\n",
    "    df = pd.read_parquet(\n",
    "        path,\n",
    "        columns=[col_name, \"target\"],\n",
    "        engine=\"pyarrow\",\n",
    "        use_nullable_dtypes=False,\n",
    "    )\n",
    "    # Keep track of the indices\n",
    "    train_index, test_index = X_train.index, X_test.index\n",
    "    # Align the dataframes and reindex in the same order\n",
    "    X_train, _ = df.filter(like=col_name).align(X_train, join=\"inner\", axis=0)\n",
    "    X_train = X_train.reindex(train_index)\n",
    "    X_test, _ = df.filter(like=col_name).align(X_test, join=\"inner\", axis=0)\n",
    "    X_test = X_test.reindex(test_index)\n",
    "    y_train, _ = df.target.align(y_train, join=\"inner\", axis=0)\n",
    "    y_train = y_train.reindex(train_index)\n",
    "    y_test, _ = df.target.align(y_test, join=\"inner\", axis=0)\n",
    "    y_test = y_test.reindex(test_index)\n",
    "    # Return the aligned data with features squeezed to remove the extra dimension if necessary\n",
    "    return X_train.squeeze(), X_test.squeeze(), y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensorflow_dataset(X_train, X_test, y_train, y_test, col_name, path, validation_split=0.2, batch_size=32):\n",
    "    # Align the splits with the corpus directly from dataframe\n",
    "    X_train, X_test, y_train, y_test = load_splits_from_parquet(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        col_name=col_name,\n",
    "        path=path,\n",
    "    )\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "        X_train, y_train, test_size=validation_split, stratify=y_train, random_state=SEED\n",
    "    )\n",
    "    # Create the tensorflow datasets for train, val and test\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train_split, y_train_split)).batch(batch_size)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val_split, y_val_split)).batch(batch_size)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)\n",
    "    # Return the tensorflow datasets\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = 0.01\n",
    "sampling = True\n",
    "test_split = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(\n",
    "    df,\n",
    "    test_split=test_split,\n",
    "    sampling=sampling,\n",
    "    proportion=proportion,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12772,), (3194,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the size of X_train and X_test\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMMENTS**:\n",
    "- Création du pipeline modulable\n",
    "- Grille de paramètres pour le vectorizer\n",
    "- Grille de paramètres pour les modèles (LG, MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = False\n",
    "if experiment:\n",
    "    with mlflow.start_run():\n",
    "        # Load the file\n",
    "        # Fit and transform the tf-idf vectorizer on the text column\n",
    "        tfidf = TfidfVectorizer(\n",
    "            ngram_range=(1, 3),\n",
    "            max_features=1000,\n",
    "            strip_accents=\"unicode\",\n",
    "        )\n",
    "        X_embed = tfidf.fit_transform(X_train[\"text\"].str.lower())\n",
    "        # Initialize the model\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        # Add the input example\n",
    "        input_logit = X_embed[0]\n",
    "        input_tfidf = X_train[\"text\"].str.lower().values[0]\n",
    "        # Perform a cross-validation\n",
    "        scores = cross_validate(\n",
    "            model, X_embed, y_train, cv=5, scoring=[\"accuracy\", \"f1\"]\n",
    "        )\n",
    "        # Your training code here...\n",
    "        model.fit(X_embed, y_train)\n",
    "        y_pred = model.predict(tfidf.transform(X_test[\"text\"].str.lower()))\n",
    "        scores = pd.DataFrame(scores).mean()\n",
    "        acc_score = accuracy_score(y_test, y_pred)\n",
    "        for metric in scores.keys():\n",
    "            mlflow.log_metric(f\"val_{metric}\", scores[metric])\n",
    "        mlflow.log_metric(\"test_accuracy\", acc_score)\n",
    "        mlflow.log_param(\"Dimension\", X_embed.shape[1])\n",
    "        mlflow.log_param(\"Tf-Idf params\", tfidf.get_params())\n",
    "        mlflow.log_param(\"Logistic Regression params\", model.get_params())\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \"logistic_regression\", input_example=input_logit\n",
    "        )\n",
    "        mlflow.sklearn.log_model(\n",
    "            tfidf, \"tfidf_vectorizer\", input_example=input_tfidf, signature=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_score(X_train, y_train, model, col, cv=10):\n",
    "    \"\"\"\n",
    "    Perform a cross-validation on the model\n",
    "    :param X_train: Training data\n",
    "    :param y_train: Training target\n",
    "    :param model: Model to train\n",
    "    :param cv: Number of folds\n",
    "    :return: scores\n",
    "    \"\"\"\n",
    "    # Perform a cross-validation\n",
    "    scores = cross_validate(model, X_train, y_train, cv=cv, scoring=[\"accuracy\", \"f1\"])\n",
    "    # Return the mean scores\n",
    "    return pd.DataFrame(scores).mean().rename(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline with Tf-Idf and Logistic Regression\n",
    "model = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"tfidf\",\n",
    "            TfidfVectorizer(\n",
    "                ngram_range=(1, 3),\n",
    "                max_features=1000,\n",
    "                strip_accents=\"unicode\",\n",
    "            ),\n",
    "        ),\n",
    "        (\"logistic\", LogisticRegression(max_iter=1000)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39malign_splits_from_df(\n\u001b[1;32m      6\u001b[0m         X_train, X_test, y_train, y_test, col\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Execute the cross-score function\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Display the scores\u001b[39;00m\n\u001b[1;32m     11\u001b[0m scores_list\u001b[38;5;241m.\u001b[39mappend(scores)\n",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m, in \u001b[0;36mcross_score\u001b[0;34m(X_train, y_train, model, col, cv)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mPerform a cross-validation on the model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m:param X_train: Training data\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m:return: scores\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Perform a cross-validation\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Return the mean scores\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(scores)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mrename(col)\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/sklearn/pipeline.py:469\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    468\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 469\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/sklearn/pipeline.py:406\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params)\u001b[0m\n\u001b[1;32m    404\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/sklearn/pipeline.py:1310\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1310\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1312\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1313\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1314\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2090\u001b[0m )\n\u001b[0;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1385\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1384\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n\u001b[0;32m-> 1385\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_limit_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_doc_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_doc_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1389\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1235\u001b[0m, in \u001b[0;36mCountVectorizer._limit_features\u001b[0;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1234\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m vocabulary[term]\n\u001b[0;32m-> 1235\u001b[0m kept_indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kept_indices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter pruning, no terms remain. Try a lower min_df or a higher max_df.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1239\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/numpy/core/multiarray.py:346\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    inner(a, b, /)\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m \n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mwhere)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhere\u001b[39m(condition, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    348\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    where(condition, [x, y], /)\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m           [ 0,  3, -1]])\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (condition, x, y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores_list = []\n",
    "for col in tqdm(cols[0][2:]):\n",
    "    # Align the data\n",
    "    if col != \"text\":\n",
    "        X_train, X_test, y_train, y_test = splitter.align_splits_from_df(\n",
    "            X_train, X_test, y_train, y_test, col\n",
    "        )\n",
    "    # Execute the cross-score function\n",
    "    scores = cross_score(X_train, y_train, model, col, cv=10)\n",
    "    # Display the scores\n",
    "    scores_list.append(scores)\n",
    "# Concatenate the scores\n",
    "scores_df = pd.concat(scores_list, axis=1)\n",
    "# Display the scores\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le retrait de certain stop words affecte probablement la capacité du modèle à prédire correctement le sentiment.<br>\n",
    "Par exemple, le mot \"not\" est un stop word et est retiré de la liste des mots à analyser. Cela peut affecter la prédiction du sentiment si le mot \"not\" est important pour la prédiction du sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline with Tf-Idf and Logistic Regression\n",
    "model = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"tfidf\",\n",
    "            TfidfVectorizer(\n",
    "                ngram_range=(1, 3),\n",
    "                max_features=1000,\n",
    "                strip_accents=\"unicode\",\n",
    "            ),\n",
    "        ),\n",
    "        (\"MultinomialNB\", MultinomialNB()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:45<00:00, 23.68s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenizer with lowercase</th>\n",
       "      <th>tokenizer with lowercase, handle stripping, and length reduction</th>\n",
       "      <th>tokenizer with lowercase and alpha</th>\n",
       "      <th>tokenizer with lowercase, alpha and emoji</th>\n",
       "      <th>tokenizer with lowercase, alpha, and no stop words</th>\n",
       "      <th>tokenizer with lowercase, alpha and emoji, and no stop words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>2.724755</td>\n",
       "      <td>2.843830</td>\n",
       "      <td>2.538568</td>\n",
       "      <td>2.455027</td>\n",
       "      <td>2.245535</td>\n",
       "      <td>1.202892</td>\n",
       "      <td>1.473164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.096113</td>\n",
       "      <td>0.099040</td>\n",
       "      <td>0.090046</td>\n",
       "      <td>0.083166</td>\n",
       "      <td>0.083293</td>\n",
       "      <td>0.051234</td>\n",
       "      <td>0.049753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.738229</td>\n",
       "      <td>0.739121</td>\n",
       "      <td>0.738229</td>\n",
       "      <td>0.737368</td>\n",
       "      <td>0.737368</td>\n",
       "      <td>0.719173</td>\n",
       "      <td>0.719173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.728693</td>\n",
       "      <td>0.730365</td>\n",
       "      <td>0.728495</td>\n",
       "      <td>0.730633</td>\n",
       "      <td>0.730633</td>\n",
       "      <td>0.710744</td>\n",
       "      <td>0.710744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   text  tokenizer with lowercase  \\\n",
       "fit_time       2.724755                  2.843830   \n",
       "score_time     0.096113                  0.099040   \n",
       "test_accuracy  0.738229                  0.739121   \n",
       "test_f1        0.728693                  0.730365   \n",
       "\n",
       "               tokenizer with lowercase, handle stripping, and length reduction  \\\n",
       "fit_time                                                2.538568                  \n",
       "score_time                                              0.090046                  \n",
       "test_accuracy                                           0.738229                  \n",
       "test_f1                                                 0.728495                  \n",
       "\n",
       "               tokenizer with lowercase and alpha  \\\n",
       "fit_time                                 2.455027   \n",
       "score_time                               0.083166   \n",
       "test_accuracy                            0.737368   \n",
       "test_f1                                  0.730633   \n",
       "\n",
       "               tokenizer with lowercase, alpha and emoji  \\\n",
       "fit_time                                        2.245535   \n",
       "score_time                                      0.083293   \n",
       "test_accuracy                                   0.737368   \n",
       "test_f1                                         0.730633   \n",
       "\n",
       "               tokenizer with lowercase, alpha, and no stop words  \\\n",
       "fit_time                                                1.202892    \n",
       "score_time                                              0.051234    \n",
       "test_accuracy                                           0.719173    \n",
       "test_f1                                                 0.710744    \n",
       "\n",
       "               tokenizer with lowercase, alpha and emoji, and no stop words  \n",
       "fit_time                                                1.473164             \n",
       "score_time                                              0.049753             \n",
       "test_accuracy                                           0.719173             \n",
       "test_f1                                                 0.710744             "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_list = []\n",
    "for col in tqdm(cols[0][2:]):\n",
    "    # Align the data\n",
    "    X_train, X_test, y_train, y_test = splitter.align_splits_from_df(\n",
    "        X_train, X_test, y_train, y_test, col\n",
    "    )\n",
    "    # Execute the cross-score function\n",
    "    scores = cross_score(X_train, y_train, model, col, cv=10)\n",
    "    # Display the scores\n",
    "    scores_list.append(scores)\n",
    "# Concatenate the scores\n",
    "scores_df = pd.concat(scores_list, axis=1)\n",
    "# Display the scores\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CUSTOM NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom tensorflow text standardization with lowercase, punctuation removal and tokenization\n",
    "def custom_standardization(tensor):\n",
    "    tensor = tf.strings.lower(tensor)  # lowercase\n",
    "    tensor = tf.strings.regex_replace(tensor, r\"@\\w+\", \" \")  # strip mentions\n",
    "    tensor = tf.strings.regex_replace(tensor, r\"http\\S+|www\\S+\", \" \")  # strip urls\n",
    "    tensor = tf.strings.regex_replace(tensor, r\"[^\\w\\s\\d]\", \" \")  # strip punctuation\n",
    "    tensor = tf.strings.regex_replace(tensor, r\"\\s{2,}\", \" \")  # strip multiple spaces\n",
    "    return tf.strings.strip(tensor)  # strip leading and trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = \"http://localhost:5000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tf_dataset(col_name=\"text\", val_split=0.2, batch_size=32):\n",
    "# Make the tf splits\n",
    "    train_ds, val_ds, test_ds = to_tensorflow_dataset(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        col_name=col_name,\n",
    "        path=PATH,\n",
    "        validation_split=val_split,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vocab, pretrained_weights, random_weights):\n",
    "    embedding_dim = pretrained_weights.vector_size\n",
    "    word_index = dict(zip(vocab, range(len(vocab))))\n",
    "    # Initialiser la matrice d'embeddings\n",
    "    match random_weights:\n",
    "        case 'normal':\n",
    "            embedding_matrix = np.random.normal(scale=0.6, size=(len(vocab), embedding_dim))\n",
    "        case 'uniform':\n",
    "            embedding_matrix = np.random.uniform(0, 1, size=(len(vocab), embedding_dim))\n",
    "        case _:\n",
    "            embedding_matrix = np.random.normal(scale=0.6, size=(len(vocab), embedding_dim))\n",
    "            print(\"Uniquement les valeurs normal ou uniform sont acceptées. Les poids ont été initialisés avec la méthode normale.\")\n",
    "    # Remplir la matrice d'embeddings avec les mots trouvés dans la vectorize layer et laisse les autres poids \n",
    "    for word, i in word_index.items():\n",
    "        if i < len(word_index):\n",
    "            try:\n",
    "                embedding_vector = pretrained_weights[word]\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "            except KeyError:\n",
    "                pass\n",
    "    return embedding_matrix, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_model(max_tokens, seq_length, embedding_dim, additionnal_layers, pretrained_weights=None, random_weights='uniform'):\n",
    "    # Create the text vectorization layer\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=seq_length,\n",
    "        standardize=custom_standardization,\n",
    "        trainable=False,\n",
    "        name=\"vectorization\",\n",
    "    )\n",
    "\n",
    "    # Adapt the text vectorization layer to the train dataset\n",
    "    vectorize_layer.adapt(train_ds.map(lambda text, label: text))\n",
    "    # Get the vocabulary\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    vocab_size = len(vocab)\n",
    "    print(\"Vocabulary size: \", vocab_size)\n",
    "    # Create the embedding layer\n",
    "    if pretrained_weights is not None:\n",
    "        embedding_matrix, embedding_dim = create_embedding_matrix(vocab, pretrained_weights, random_weights)\n",
    "    # Create the embedding layer\n",
    "    model = tf.keras.Sequential([\n",
    "        vectorize_layer,\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            weights=[embedding_matrix] if pretrained_weights is not None else None,\n",
    "            input_length=seq_length,\n",
    "            embeddings_initializer=random_weights,\n",
    "            trainable=True,\n",
    "            name=\"embedding\",\n",
    "            ),\n",
    "        *additionnal_layers,\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "            loss=tf.losses.BinaryCrossentropy(),\n",
    "            optimizer=\"adam\",\n",
    "            metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)],\n",
    "        )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/951305245308831132', creation_time=1731164356530, experiment_id='951305245308831132', last_update_time=1731164356530, lifecycle_stage='active', name='neural_network_scratch_embedding', tags={}>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the name of your experiment\n",
    "experiment = 'neural_network_scratch_embedding'\n",
    "\n",
    "# Set the tracking URI\n",
    "mlflow.set_tracking_uri(URI)\n",
    "    # try to connect to the server\n",
    "try:\n",
    "    mlflow.tracking.get_tracking_uri()\n",
    "except Exception as e:\n",
    "    print(f\"Cannot connect to the server : {URI}. Check the server status.\")\n",
    "    raise e\n",
    "# Set, and create if necessary, the experiment\n",
    "try:\n",
    "    mlflow.create_experiment(experiment)\n",
    "except:\n",
    "    pass\n",
    "mlflow.set_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args for dataset preparation\n",
    "col_name = \"text\"\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "# Create the datasets\n",
    "train_ds, val_ds, test_ds = prepare_tf_dataset(col_name, val_split, batch_size)\n",
    "\n",
    "# Args for the model\n",
    "max_tokens = 1000\n",
    "seq_length = 100\n",
    "embedding_dim = 16\n",
    "embedding_trainable = True\n",
    "epochs = 3\n",
    "additionnal_layers = [(tf.keras.layers.GlobalAveragePooling1D(),), (tf.keras.layers.GlobalMaxPooling1D(),),]\n",
    "runs = (\"GlobalAveragePooling1D\", \"GlobalMaxPooling1D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:27:45.711055: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024/11/09 17:27:46 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xef in position 31: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  1000\n",
      "Epoch 1/3\n",
      "  7/320 [..............................] - ETA: 2s - loss: 0.6936 - binary_accuracy: 0.5089  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:27:46.667590: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:27:46.713638: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:27:46.713678: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:27:46.716256: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:27:46.716287: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:27:46.717996: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:27:46.718027: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318/320 [============================>.] - ETA: 0s - loss: 0.6915 - binary_accuracy: 0.5615"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:27:48.994936: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:27:49.011558: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:27:49.011603: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6915 - binary_accuracy: 0.5622 - val_loss: 0.6886 - val_binary_accuracy: 0.6720\n",
      "Epoch 2/3\n",
      "320/320 [==============================] - 2s 8ms/step - loss: 0.6836 - binary_accuracy: 0.6420 - val_loss: 0.6775 - val_binary_accuracy: 0.6681\n",
      "Epoch 3/3\n",
      "320/320 [==============================] - 2s 8ms/step - loss: 0.6683 - binary_accuracy: 0.6658 - val_loss: 0.6601 - val_binary_accuracy: 0.6732\n",
      "1/1 [==============================] - 0s 156ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:27:54.325675: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8cc6q6d_/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 17:27:58 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp8cc6q6d_/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "==============\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.6613 - binary_accuracy: 0.6722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 17:27:59 INFO mlflow.tracking._tracking_service.client: 🏃 View run GlobalAveragePooling1D at: http://localhost:5000/#/experiments/951305245308831132/runs/73f80927f1ae4117ab353e77f30de269.\n",
      "2024/11/09 17:27:59 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/951305245308831132.\n",
      "2024-11-09 17:27:59.591506: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024/11/09 17:28:00 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xef in position 31: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  1000\n",
      "Epoch 1/3\n",
      " 14/320 [>.............................] - ETA: 2s - loss: 0.6926 - binary_accuracy: 0.4888 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:28:00.421205: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:28:00.462793: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:28:00.462840: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:28:00.464422: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:28:00.464454: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:28:00.466036: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:28:00.466076: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/320 [============================>.] - ETA: 0s - loss: 0.6766 - binary_accuracy: 0.6485"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:28:02.647433: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:28:02.664562: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:28:02.664606: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 3s 8ms/step - loss: 0.6763 - binary_accuracy: 0.6493 - val_loss: 0.6472 - val_binary_accuracy: 0.7014\n",
      "Epoch 2/3\n",
      "320/320 [==============================] - 2s 7ms/step - loss: 0.6078 - binary_accuracy: 0.7262 - val_loss: 0.5736 - val_binary_accuracy: 0.7303\n",
      "Epoch 3/3\n",
      "320/320 [==============================] - 2s 7ms/step - loss: 0.5386 - binary_accuracy: 0.7577 - val_loss: 0.5351 - val_binary_accuracy: 0.7436\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:28:07.776313: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpyt5pdnh4/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 17:28:09 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpyt5pdnh4/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "==============\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.5320 - binary_accuracy: 0.7483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 17:28:11 INFO mlflow.tracking._tracking_service.client: 🏃 View run GlobalMaxPooling1D at: http://localhost:5000/#/experiments/951305245308831132/runs/63957ca1dc064e2a9b4bd32b7b6dabd0.\n",
      "2024/11/09 17:28:11 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/951305245308831132.\n"
     ]
    }
   ],
   "source": [
    "for layers, run_name in zip(additionnal_layers, runs):\n",
    "    # Create the model\n",
    "    model = create_tf_model(max_tokens, seq_length, embedding_dim, layers)\n",
    "    model.get_layer(\"embedding\").trainable = embedding_trainable\n",
    "    mlflow.tensorflow.autolog(checkpoint=False, log_models=True)\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Fit the model\n",
    "        history = model.fit(\n",
    "                    train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                )\n",
    "        mlflow.log_param(\"batch_size_\", batch_size)\n",
    "        mlflow.log_param(\"validation_split_\",val_split)\n",
    "\n",
    "        # Evaluate the model\n",
    "        print(\"Evaluate on test data\")\n",
    "        print(\"==============\")\n",
    "        loss, accuracy = model.evaluate(test_ds)\n",
    "        mlflow.log_metric(\"test_loss\", loss)\n",
    "        mlflow.log_metric(\"test_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args for dataset preparation\n",
    "col_name = \"text\"\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "# Create the datasets\n",
    "train_ds, val_ds, test_ds = prepare_tf_dataset(col_name, val_split, batch_size)\n",
    "\n",
    "# Args for the model\n",
    "max_tokens = None\n",
    "seq_length = 100\n",
    "embedding_dim = 16\n",
    "embedding_trainable = True\n",
    "epochs = 3\n",
    "additionnal_layers = [(tf.keras.layers.GlobalAveragePooling1D(),), (tf.keras.layers.GlobalMaxPooling1D(),),]\n",
    "runs = (\"GlobalAveragePooling1D with max tokens up to vocab size\", \"GlobalMaxPooling1D with max tokens up to vocab size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:28:59.075788: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024/11/09 17:28:59 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xef in position 31: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  13729\n",
      "Epoch 1/3\n",
      "  1/320 [..............................] - ETA: 1:48 - loss: 0.6950 - binary_accuracy: 0.4375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:28:59.928870: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:28:59.971806: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:28:59.971855: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:28:59.973167: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:28:59.973206: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:28:59.974946: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:28:59.974978: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/320 [..............................] - ETA: 7s - loss: 0.6937 - binary_accuracy: 0.4922  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0224s). Check your callbacks.\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.6913 - binary_accuracy: 0.5700"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:29:07.570870: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:29:07.587736: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:29:07.587782: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 8s 24ms/step - loss: 0.6913 - binary_accuracy: 0.5700 - val_loss: 0.6883 - val_binary_accuracy: 0.6638\n",
      "Epoch 2/3\n",
      "320/320 [==============================] - 8s 24ms/step - loss: 0.6819 - binary_accuracy: 0.6537 - val_loss: 0.6760 - val_binary_accuracy: 0.6669\n",
      "Epoch 3/3\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.6624 - binary_accuracy: 0.6904 - val_loss: 0.6563 - val_binary_accuracy: 0.6716\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:29:26.333448: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptsb1qdda/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 17:29:28 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmptsb1qdda/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "==============\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.6574 - binary_accuracy: 0.6788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 17:29:29 INFO mlflow.tracking._tracking_service.client: 🏃 View run GlobalAveragePooling1D with max tokens up to vocab size at: http://localhost:5000/#/experiments/951305245308831132/runs/9c477df74ee6490094ef19ef2b20d38d.\n",
      "2024/11/09 17:29:29 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/951305245308831132.\n",
      "2024-11-09 17:29:29.714557: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024/11/09 17:29:30 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xef in position 31: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  13729\n",
      "Epoch 1/3\n",
      "  4/320 [..............................] - ETA: 7s - loss: 0.6934 - binary_accuracy: 0.5078  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0203s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:29:30.540994: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:29:30.582925: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:29:30.582963: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:29:30.584100: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:29:30.584131: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:29:30.585755: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:29:30.585793: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - ETA: 0s - loss: 0.6801 - binary_accuracy: 0.6285"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:29:38.233578: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:29:38.250506: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:29:38.250541: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 8s 25ms/step - loss: 0.6801 - binary_accuracy: 0.6285 - val_loss: 0.6574 - val_binary_accuracy: 0.7022\n",
      "Epoch 2/3\n",
      "320/320 [==============================] - 8s 24ms/step - loss: 0.6187 - binary_accuracy: 0.7365 - val_loss: 0.5903 - val_binary_accuracy: 0.7252\n",
      "Epoch 3/3\n",
      "320/320 [==============================] - 8s 24ms/step - loss: 0.5378 - binary_accuracy: 0.7845 - val_loss: 0.5448 - val_binary_accuracy: 0.7425\n",
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:29:53.784869: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpjbuo7wla/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 17:29:56 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpjbuo7wla/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "==============\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.5414 - binary_accuracy: 0.7436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 17:29:57 INFO mlflow.tracking._tracking_service.client: 🏃 View run GlobalMaxPooling1D with max tokens up to vocab size at: http://localhost:5000/#/experiments/951305245308831132/runs/0aab36560f8e41c281fd1e9a986a9799.\n",
      "2024/11/09 17:29:57 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/951305245308831132.\n"
     ]
    }
   ],
   "source": [
    "for layers, run_name in zip(additionnal_layers, runs):\n",
    "    # Create the model\n",
    "    model = create_tf_model(max_tokens, seq_length, embedding_dim, layers)\n",
    "    model.get_layer(\"embedding\").trainable = embedding_trainable\n",
    "    mlflow.tensorflow.autolog(checkpoint=False, log_models=True)\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Fit the model\n",
    "        history = model.fit(\n",
    "                    train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                )\n",
    "        mlflow.log_param(\"batch_size_\", batch_size)\n",
    "        mlflow.log_param(\"validation_split_\",val_split)\n",
    "\n",
    "        # Evaluate the model\n",
    "        print(\"Evaluate on test data\")\n",
    "        print(\"==============\")\n",
    "        loss, accuracy = model.evaluate(test_ds)\n",
    "        mlflow.log_metric(\"test_loss\", loss)\n",
    "        mlflow.log_metric(\"test_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args for dataset preparation\n",
    "col_name = \"text\"\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "# Create the datasets\n",
    "train_ds, val_ds, test_ds = prepare_tf_dataset(col_name, val_split, batch_size)\n",
    "\n",
    "# Args for the model\n",
    "max_tokens = 5000\n",
    "seq_length = 100\n",
    "embedding_dim = 16\n",
    "embedding_trainable = True\n",
    "epochs = 3\n",
    "additionnal_layers = [(tf.keras.layers.GlobalAveragePooling1D(),), (tf.keras.layers.GlobalMaxPooling1D(),),]\n",
    "runs = (\"GlobalAveragePooling1D\", \"GlobalMaxPooling1D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:30:26.817384: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024/11/09 17:30:27 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xef in position 31: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5000\n",
      "Epoch 1/3\n",
      "  6/320 [..............................] - ETA: 3s - loss: 0.6927 - binary_accuracy: 0.5156  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0044s vs `on_train_batch_end` time: 0.0079s). Check your callbacks.\n",
      " 11/320 [>.............................] - ETA: 3s - loss: 0.6933 - binary_accuracy: 0.4972"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:30:27.688117: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:30:27.731275: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:30:27.731323: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:30:27.732863: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:30:27.732895: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:30:27.734968: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:30:27.735005: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/320 [============================>.] - ETA: 0s - loss: 0.6913 - binary_accuracy: 0.5569"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:30:31.361600: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:30:31.378384: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:30:31.378421: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 4s 12ms/step - loss: 0.6913 - binary_accuracy: 0.5584 - val_loss: 0.6883 - val_binary_accuracy: 0.6544\n",
      "Epoch 2/3\n",
      "320/320 [==============================] - 7s 21ms/step - loss: 0.6820 - binary_accuracy: 0.6474 - val_loss: 0.6753 - val_binary_accuracy: 0.6658\n",
      "Epoch 3/3\n",
      "320/320 [==============================] - 4s 12ms/step - loss: 0.6628 - binary_accuracy: 0.6832 - val_loss: 0.6551 - val_binary_accuracy: 0.6728\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb76e06fa30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:30:42.458121: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpdc2n8q85/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 17:30:44 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpdc2n8q85/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "==============\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.6565 - binary_accuracy: 0.6760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 17:30:45 INFO mlflow.tracking._tracking_service.client: 🏃 View run GlobalAveragePooling1D at: http://localhost:5000/#/experiments/951305245308831132/runs/2e98affd63eb49b08dc85d91082eb816.\n",
      "2024/11/09 17:30:45 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/951305245308831132.\n",
      "2024-11-09 17:30:45.843025: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024/11/09 17:30:46 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xef in position 31: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5000\n",
      "Epoch 1/3\n",
      "  6/320 [..............................] - ETA: 3s - loss: 0.6926 - binary_accuracy: 0.5156  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_train_batch_end` time: 0.0081s). Check your callbacks.\n",
      " 11/320 [>.............................] - ETA: 3s - loss: 0.6938 - binary_accuracy: 0.4943"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:30:46.683277: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:30:46.725649: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:30:46.725690: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:30:46.726960: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:30:46.726991: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:30:46.728542: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:30:46.728573: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/320 [============================>.] - ETA: 0s - loss: 0.6795 - binary_accuracy: 0.6196"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:30:50.986825: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:30:51.003672: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:30:51.003715: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 5s 14ms/step - loss: 0.6791 - binary_accuracy: 0.6208 - val_loss: 0.6548 - val_binary_accuracy: 0.7065\n",
      "Epoch 2/3\n",
      "320/320 [==============================] - 4s 12ms/step - loss: 0.6225 - binary_accuracy: 0.7283 - val_loss: 0.5908 - val_binary_accuracy: 0.7346\n",
      "Epoch 3/3\n",
      "320/320 [==============================] - 4s 12ms/step - loss: 0.5495 - binary_accuracy: 0.7695 - val_loss: 0.5449 - val_binary_accuracy: 0.7405\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb90ebbac20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:30:59.112651: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpohs4si10/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 17:31:01 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpohs4si10/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "==============\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.5430 - binary_accuracy: 0.7436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 17:31:02 INFO mlflow.tracking._tracking_service.client: 🏃 View run GlobalMaxPooling1D at: http://localhost:5000/#/experiments/951305245308831132/runs/f25d1e2563a34c78b0ad86e5a0cb609b.\n",
      "2024/11/09 17:31:02 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/951305245308831132.\n"
     ]
    }
   ],
   "source": [
    "for layers, run_name in zip(additionnal_layers, runs):\n",
    "    # Create the model\n",
    "    model = create_tf_model(max_tokens, seq_length, embedding_dim, layers)\n",
    "    model.get_layer(\"embedding\").trainable = embedding_trainable\n",
    "    mlflow.tensorflow.autolog(checkpoint=False, log_models=True)\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Fit the model\n",
    "        history = model.fit(\n",
    "                    train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                )\n",
    "        mlflow.log_param(\"batch_size_\", batch_size)\n",
    "        mlflow.log_param(\"validation_split_\",val_split)\n",
    "\n",
    "        # Evaluate the model\n",
    "        print(\"Evaluate on test data\")\n",
    "        print(\"==============\")\n",
    "        loss, accuracy = model.evaluate(test_ds)\n",
    "        mlflow.log_metric(\"test_loss\", loss)\n",
    "        mlflow.log_metric(\"test_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GLOVE EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/951305245308831132', creation_time=1731164356530, experiment_id='951305245308831132', last_update_time=1731164356530, lifecycle_stage='active', name='neural_network_scratch_embedding', tags={}>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the name of your experiment\n",
    "experiment = 'neural_network_scratch_embedding'\n",
    "\n",
    "# Set the tracking URI\n",
    "mlflow.set_tracking_uri(URI)\n",
    "    # try to connect to the server\n",
    "try:\n",
    "    mlflow.tracking.get_tracking_uri()\n",
    "except Exception as e:\n",
    "    print(f\"Cannot connect to the server : {URI}. Check the server status.\")\n",
    "    raise e\n",
    "# Set, and create if necessary, the experiment\n",
    "try:\n",
    "    mlflow.create_experiment(experiment)\n",
    "except:\n",
    "    pass\n",
    "mlflow.set_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the glove-twitter-100 model\n",
    "repo_id = \"fse/glove-twitter-100\"\n",
    "model_file = hf_hub_download(repo_id=repo_id, filename=\"glove-twitter-100.model\")\n",
    "vector_file = hf_hub_download(\n",
    "    repo_id=repo_id, filename=\"glove-twitter-100.model.vectors.npy\"\n",
    ")\n",
    "glove = KeyedVectors.load(model_file, mmap=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args for dataset preparation\n",
    "col_name = \"text\"\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "# Create the datasets\n",
    "train_ds, val_ds, test_ds = prepare_tf_dataset(col_name, val_split, batch_size)\n",
    "\n",
    "# Args for the model\n",
    "max_tokens = 5000\n",
    "seq_length = 100\n",
    "embedding_dim = 16\n",
    "embedding_trainable = False\n",
    "epochs = 3\n",
    "additionnal_layers = [(tf.keras.layers.GlobalAveragePooling1D(),), (tf.keras.layers.GlobalMaxPooling1D(),),]\n",
    "runs = (\"GlobalAveragePooling1D\", \"GlobalMaxPooling1D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:32:09.354785: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024/11/09 17:32:10 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xef in position 31: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5000\n",
      "Epoch 1/3\n",
      "  3/320 [..............................] - ETA: 13s - loss: 0.8653 - binary_accuracy: 0.4687 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:32:10.498941: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:32:10.543432: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:32:10.543473: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:32:10.544784: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:32:10.544815: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:32:10.546360: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:32:10.546391: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/320 [..............................] - ETA: 13s - loss: 0.8371 - binary_accuracy: 0.4813WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0048s vs `on_train_batch_end` time: 0.0381s). Check your callbacks.\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.6958 - binary_accuracy: 0.5181"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:32:26.786013: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:32:26.803675: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:32:26.803723: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 17s 52ms/step - loss: 0.6958 - binary_accuracy: 0.5181 - val_loss: 0.6860 - val_binary_accuracy: 0.5147\n",
      "Epoch 2/3\n",
      "320/320 [==============================] - 14s 42ms/step - loss: 0.6788 - binary_accuracy: 0.6008 - val_loss: 0.6749 - val_binary_accuracy: 0.5804\n",
      "Epoch 3/3\n",
      "320/320 [==============================] - 19s 61ms/step - loss: 0.6641 - binary_accuracy: 0.6558 - val_loss: 0.6612 - val_binary_accuracy: 0.6067\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:33:00.053878: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpfkal15b2/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 17:33:02 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpfkal15b2/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "==============\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.6622 - binary_accuracy: 0.6115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 17:33:03 INFO mlflow.tracking._tracking_service.client: 🏃 View run GlobalAveragePooling1D at: http://localhost:5000/#/experiments/951305245308831132/runs/0b8318ce382349018cb2079fac4f7681.\n",
      "2024/11/09 17:33:03 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/951305245308831132.\n",
      "2024-11-09 17:33:03.445239: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024/11/09 17:33:04 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xef in position 31: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5000\n",
      "Epoch 1/3\n",
      "  3/320 [..............................] - ETA: 12s - loss: 0.9958 - binary_accuracy: 0.5312 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:33:04.315455: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:33:04.360632: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:33:04.360679: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:33:04.361854: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:33:04.361886: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 17:33:04.363489: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:33:04.363522: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/320 [..............................] - ETA: 13s - loss: 0.9934 - binary_accuracy: 0.5188WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0046s vs `on_train_batch_end` time: 0.0389s). Check your callbacks.\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.6838 - binary_accuracy: 0.5770"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:33:17.562021: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 17:33:17.578892: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 17:33:17.578938: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 14s 42ms/step - loss: 0.6838 - binary_accuracy: 0.5769 - val_loss: 0.6561 - val_binary_accuracy: 0.6043\n",
      "Epoch 2/3\n",
      "320/320 [==============================] - 29s 90ms/step - loss: 0.6257 - binary_accuracy: 0.6747 - val_loss: 0.6139 - val_binary_accuracy: 0.6658\n",
      "Epoch 3/3\n",
      "286/320 [=========================>....] - ETA: 6s - loss: 0.5737 - binary_accuracy: 0.7327"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for layers, run_name in zip(additionnal_layers, runs):\n",
    "    # Create the model\n",
    "    model = create_tf_model(max_tokens, seq_length, embedding_dim, layers, pretrained_weights=glove)\n",
    "    # set the embedding layer trainable or not\n",
    "    model.get_layer(\"embedding\").trainable = embedding_trainable\n",
    "    mlflow.tensorflow.autolog(checkpoint=False, log_models=True)\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Fit the model\n",
    "        history = model.fit(\n",
    "                    train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                )\n",
    "        mlflow.log_param(\"batch_size_\", batch_size)\n",
    "        mlflow.log_param(\"validation_split_\",val_split)\n",
    "\n",
    "        # Evaluate the model\n",
    "        print(\"Evaluate on test data\")\n",
    "        print(\"==============\")\n",
    "        loss, accuracy = model.evaluate(test_ds)\n",
    "        mlflow.log_metric(\"test_loss\", loss)\n",
    "        mlflow.log_metric(\"test_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args for dataset preparation\n",
    "col_name = \"text\"\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "# Create the datasets\n",
    "train_ds, val_ds, test_ds = prepare_tf_dataset(col_name, val_split, batch_size)\n",
    "\n",
    "# Args for the model\n",
    "max_tokens = 5000\n",
    "seq_length = 100\n",
    "embedding_dim = 16\n",
    "embedding_trainable = True\n",
    "epochs = 3\n",
    "additionnal_layers = [(tf.keras.layers.GlobalAveragePooling1D(),), (tf.keras.layers.GlobalMaxPooling1D(),),]\n",
    "runs = (\"GlobalAveragePooling1D\", \"GlobalMaxPooling1D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 16:55:43.967624: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024/11/09 16:55:44 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xef in position 31: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  1000\n",
      "Epoch 1/3\n",
      "  6/320 [..............................] - ETA: 3s - loss: 0.7330 - binary_accuracy: 0.5156  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_train_batch_end` time: 0.0081s). Check your callbacks.\n",
      " 11/320 [>.............................] - ETA: 3s - loss: 0.7257 - binary_accuracy: 0.4972"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 16:55:45.084136: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 16:55:45.133485: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 16:55:45.133528: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 16:55:45.134877: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 16:55:45.134910: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 16:55:45.136487: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 16:55:45.136520: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/320 [============================>.] - ETA: 0s - loss: 0.6885 - binary_accuracy: 0.5475"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 16:55:48.802232: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 16:55:48.819865: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 16:55:48.819908: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 4s 12ms/step - loss: 0.6884 - binary_accuracy: 0.5479 - val_loss: 0.6856 - val_binary_accuracy: 0.5401\n",
      "Epoch 2/3\n",
      "320/320 [==============================] - 4s 12ms/step - loss: 0.6777 - binary_accuracy: 0.5873 - val_loss: 0.6761 - val_binary_accuracy: 0.5656\n",
      "Epoch 3/3\n",
      "320/320 [==============================] - 7s 21ms/step - loss: 0.6660 - binary_accuracy: 0.6206 - val_loss: 0.6631 - val_binary_accuracy: 0.5930\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 16:55:59.869873: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp3kbr91p3/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 16:56:01 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp3kbr91p3/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "==============\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.6634 - binary_accuracy: 0.6099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 16:56:03 INFO mlflow.tracking._tracking_service.client: 🏃 View run GlobalAveragePooling1D at: http://localhost:5000/#/experiments/951305245308831132/runs/ddefa0f6302040e1b317999eb90669a5.\n",
      "2024/11/09 16:56:03 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/951305245308831132.\n",
      "2024-11-09 16:56:03.244618: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024/11/09 16:56:03 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xef in position 31: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  1000\n",
      "Epoch 1/3\n",
      "  6/320 [..............................] - ETA: 3s - loss: 0.6935 - binary_accuracy: 0.5156  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0046s vs `on_train_batch_end` time: 0.0074s). Check your callbacks.\n",
      " 11/320 [>.............................] - ETA: 3s - loss: 0.6937 - binary_accuracy: 0.5170"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 16:56:04.079820: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 16:56:04.124221: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 16:56:04.124262: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 16:56:04.125386: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 16:56:04.125417: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-09 16:56:04.127212: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 16:56:04.127244: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/320 [============================>.] - ETA: 0s - loss: 0.6771 - binary_accuracy: 0.5763"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 16:56:07.796411: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-09 16:56:07.813443: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 16:56:07.813477: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 4s 12ms/step - loss: 0.6767 - binary_accuracy: 0.5776 - val_loss: 0.6572 - val_binary_accuracy: 0.6344\n",
      "Epoch 2/3\n",
      "320/320 [==============================] - 4s 12ms/step - loss: 0.6379 - binary_accuracy: 0.6738 - val_loss: 0.6197 - val_binary_accuracy: 0.6724\n",
      "Epoch 3/3\n",
      "320/320 [==============================] - 15s 45ms/step - loss: 0.5952 - binary_accuracy: 0.7137 - val_loss: 0.5814 - val_binary_accuracy: 0.7065\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 16:56:26.519799: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4yh_3x3x/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 16:56:28 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp4yh_3x3x/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "==============\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.5858 - binary_accuracy: 0.7116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/09 16:56:29 INFO mlflow.tracking._tracking_service.client: 🏃 View run GlobalMaxPooling1D at: http://localhost:5000/#/experiments/951305245308831132/runs/cb952c0c63944d219ee441ec9a646dcf.\n",
      "2024/11/09 16:56:29 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/951305245308831132.\n"
     ]
    }
   ],
   "source": [
    "for layers, run_name in zip(additionnal_layers, runs):\n",
    "    # Create the model\n",
    "    model = create_tf_model(max_tokens, seq_length, embedding_dim, layers, pretrained_weights=glove)\n",
    "    model.get_layer(\"embedding\").trainable = embedding_trainable\n",
    "    mlflow.tensorflow.autolog(checkpoint=False, log_models=True)\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Fit the model\n",
    "        history = model.fit(\n",
    "                    train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                )\n",
    "        mlflow.log_param(\"batch_size_\", batch_size)\n",
    "        mlflow.log_param(\"validation_split_\",val_split)\n",
    "\n",
    "        # Evaluate the model\n",
    "        print(\"Evaluate on test data\")\n",
    "        print(\"==============\")\n",
    "        loss, accuracy = model.evaluate(test_ds)\n",
    "        mlflow.log_metric(\"test_loss\", loss)\n",
    "        mlflow.log_metric(\"test_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FASTTEXT EMBEDDINGS + CUSTOM NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fasttext embeddings trained on twitter data\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\"\n",
    ")\n",
    "model = fasttext.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tester avec couches CONV1D\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=maxlen, \n",
    "                           trainable=False))\n",
    "model.add(layers.Conv1D(num_filters, kernel_size, activation))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(2500, embed_dim,input_length = X.shape[1], dropout = 0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for variable-length sequences of integers\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = layers.Embedding(max_features, 128)(inputs)\n",
    "# Add 2 bidirectional LSTMs\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = len(model[\"hello\"])\n",
    "max_features = len(model.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 300\n",
      "Max features: 2000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding dimension:\", embedding_dim)\n",
    "print(\"Max features:\", max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000, 300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(model.words):\n",
    "    embedding_matrix[i] = model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 19:27:20.200159: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-01 19:27:20.203769: I tensorflow/c/logging.cc:34] DirectML: creating device on adapter 0 (AMD Radeon RX 6700 XT)\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "2024-11-01 19:27:20.700907: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-01 19:27:20.701647: W tensorflow/core/common_runtime/pluggable_device/pluggable_device_bfc_allocator.cc:28] Overriding allow_growth setting because force_memory_growth was requested by the device.\n",
      "2024-11-01 19:27:20.702266: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "col_name = \"text\"\n",
    "train_ds, val_ds, test_ds = tf_ds_split(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    col_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32,), dtype=string, numpy=\n",
      "array([b\"I love asparagus. I just thouht I'd express my vegetable love seeing as Dad's watching about it on The One Show \",\n",
      "       b'arrrrhhh, i did it again i past 100 and now 200 i was gunna say it was my 200th update but i didnt get to again ',\n",
      "       b\"Losing one's phone in the house while it's set to &quot;silent&quot; is not the best. \",\n",
      "       b'how do you deal with a long distance relationship help??? ',\n",
      "       b'BORED of packing ',\n",
      "       b\"@duckierenee I've just read that, you're EVIL and when are you coming to cook for me? \",\n",
      "       b\"@iphone_dev Not having a Mac means I can't use pwnage so  can't get the Egg \",\n",
      "       b'@wickedgirl24 glad you like it ',\n",
      "       b'I am feeling like Donald Trump today.  This morning needs to get in line.  ',\n",
      "       b\"@PleaseBiteMe It's totally fair \",\n",
      "       b\"anti-gay protesters dwntwn make mooncheez sad  we support 1st amendmnt rights, but don't agree with the msg. everyone should have rights!\",\n",
      "       b'@Saphira_blue Good. You are progressing mashallah ',\n",
      "       b\"@alyssagreco Don't actually, I would share with you but I am no where near you \",\n",
      "       b'@dunkindonuts_ yeah bubu   *ouch* where were you?? i was waitin',\n",
      "       b\"@fiiiwhore don't fail  goood luckkkk\",\n",
      "       b'@simondoggett @katematlock  so far sofa is looking like a DFS job and the bed... Well I had a few suggestions on that!',\n",
      "       b'i am bored out of mind during summer 09 ',\n",
      "       b\"Small favors: on a full flight, it seems I have an aisle with one of the only free middle seats.  let's hope it stays that way.\",\n",
      "       b'Post mortem ',\n",
      "       b'Ugh, me not being able to type properly = too early for twitter!! The gym opens in 23 mins!! Bye Tweetie Pies ',\n",
      "       b'@JohnLloydTaylor I started screaming when I saw you on JONAS. It was so cool! You rock  &lt;3',\n",
      "       b\"I have a migraine, I'm nauseous, my hands are shaking, and I'm just realising how nice it used to be to have someone rub my back \",\n",
      "       b'Morning Tweets ', b'Just got in to work  ',\n",
      "       b\"Can't wait for tomorrow! \",\n",
      "       b'There are just sooo many articles to write. Goldie, missed you alr! Should have just go eat with you n xiaocui!  - http://tweet.sg',\n",
      "       b'last time washing my hair ',\n",
      "       b\"ahh, i'm bored! don't want to go to school tomorrow  xxxx\",\n",
      "       b'Been in my bed sick all day long.  fixing to attempt to eat something for the first time today.',\n",
      "       b\"@findingurstyle yeah tonight's a no no \",\n",
      "       b\"Just watched ''The notebook'' again \",\n",
      "       b'Putting new songs on my shuffle, and syncing them. '],\n",
      "      dtype=object)>, <tf.Tensor: shape=(32,), dtype=int8, numpy=\n",
      "array([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "       1, 0, 1, 0, 0, 0, 0, 0, 1, 1], dtype=int8)>)\n"
     ]
    }
   ],
   "source": [
    "for row in train_ds.take(1):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 19:27:26.075378: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "sequence_length = 100\n",
    "embedding_dim = embedding_dim\n",
    "\n",
    "# Create a text datagenerator for the training set\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_sequence_length=sequence_length,\n",
    "    output_mode=\"int\",\n",
    ")\n",
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)\n",
    "\n",
    "# Map the vectorize function to the train, val and test datasets\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "val_ds = val_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Embedding(\n",
    "                input_dim=max_features,\n",
    "                output_dim=embedding_dim,\n",
    "                weights=[embedding_matrix],\n",
    "                trainable=False,\n",
    "                input_length=sequence_length,\n",
    "            ),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.losses.BinaryCrossentropy(),\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f\"../weights/{col_name}_model.h5\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  19/3194 [..............................] - ETA: 18s - loss: 0.6929 - binary_accuracy: 0.5329"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 19:13:25.634692: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-01 19:13:25.677225: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-01 19:13:25.677281: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3188/3194 [============================>.] - ETA: 0s - loss: 0.6920 - binary_accuracy: 0.5206"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 19:13:43.173326: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-01 19:13:43.187808: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-01 19:13:43.187853: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-01 19:13:44.955777: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1200000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/engine/training.py:1624\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n",
      "\u001b[1;32m   1619\u001b[0m     val_logs \u001b[38;5;241m=\u001b[39m {\n",
      "\u001b[1;32m   1620\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n",
      "\u001b[1;32m   1621\u001b[0m     }\n",
      "\u001b[1;32m   1622\u001b[0m     epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "\u001b[0;32m-> 1624\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_logs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1625\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m epoch_logs\n",
      "\u001b[1;32m   1626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/callbacks.py:448\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n",
      "\u001b[1;32m    446\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_logs(logs)\n",
      "\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n",
      "\u001b[0;32m--> 448\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/callbacks.py:1463\u001b[0m, in \u001b[0;36mModelCheckpoint.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n",
      "\u001b[1;32m   1460\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs_since_last_save \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/callbacks.py:1528\u001b[0m, in \u001b[0;36mModelCheckpoint._save_model\u001b[0;34m(self, epoch, batch, logs)\u001b[0m\n",
      "\u001b[1;32m   1522\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_weights(\n",
      "\u001b[1;32m   1523\u001b[0m             filepath,\n",
      "\u001b[1;32m   1524\u001b[0m             overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[1;32m   1525\u001b[0m             options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options,\n",
      "\u001b[1;32m   1526\u001b[0m         )\n",
      "\u001b[1;32m   1527\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1528\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1529\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1530\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1531\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1533\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/engine/training.py:2698\u001b[0m, in \u001b[0;36mModel.save\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n",
      "\u001b[1;32m   2643\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n",
      "\u001b[1;32m   2644\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\n",
      "\u001b[1;32m   2645\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   2652\u001b[0m     save_traces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[1;32m   2653\u001b[0m ):\n",
      "\u001b[1;32m   2655\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Saves the model to Tensorflow SavedModel or a single HDF5 file.\u001b[39;00m\n",
      "\u001b[1;32m   2656\u001b[0m \n",
      "\u001b[1;32m   2657\u001b[0m \u001b[38;5;124;03m    Please see `tf.keras.models.save_model` or the\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   2695\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n",
      "\u001b[1;32m   2696\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m-> 2698\u001b[0m     \u001b[43msave\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   2699\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2702\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_optimizer\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2703\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2704\u001b[0m \u001b[43m        \u001b[49m\u001b[43msignatures\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2705\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2706\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_traces\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2707\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/saving/save.py:161\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n",
      "\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_is_graph_network \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n",
      "\u001b[1;32m    151\u001b[0m         model, sequential\u001b[38;5;241m.\u001b[39mSequential\n",
      "\u001b[1;32m    152\u001b[0m     ):\n",
      "\u001b[1;32m    153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n",
      "\u001b[1;32m    154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving the model to HDF5 format requires the model to be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunctional model or a Sequential model. It does not work for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    159\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetting save_format=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) or using `save_weights`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;32m    160\u001b[0m         )\n",
      "\u001b[0;32m--> 161\u001b[0m     \u001b[43mhdf5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model_to_hdf5\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_optimizer\u001b[49m\n",
      "\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m generic_utils\u001b[38;5;241m.\u001b[39mSharedObjectSavingScope():\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/saving/hdf5_format.py:126\u001b[0m, in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n",
      "\u001b[1;32m    123\u001b[0m         f\u001b[38;5;241m.\u001b[39mattrs[k] \u001b[38;5;241m=\u001b[39m v\n",
      "\u001b[1;32m    125\u001b[0m model_weights_group \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mcreate_group(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m--> 126\u001b[0m \u001b[43msave_weights_to_hdf5_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_weights_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# TODO(b/128683857): Add integration tests between tf.keras and external\u001b[39;00m\n",
      "\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Keras, to avoid breaking TF.js users.\u001b[39;00m\n",
      "\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model\u001b[38;5;241m.\u001b[39moptimizer, optimizer_experimental\u001b[38;5;241m.\u001b[39mOptimizer):\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/saving/hdf5_format.py:755\u001b[0m, in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, model)\u001b[0m\n",
      "\u001b[1;32m    753\u001b[0m     g \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mcreate_group(layer\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[1;32m    754\u001b[0m     weights \u001b[38;5;241m=\u001b[39m _legacy_weights(layer)\n",
      "\u001b[0;32m--> 755\u001b[0m     \u001b[43msave_subset_weights_to_hdf5_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    756\u001b[0m weights \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_trainable_weights \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39m_non_trainable_weights\n",
      "\u001b[1;32m    757\u001b[0m g \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mcreate_group(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_level_model_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/saving/hdf5_format.py:723\u001b[0m, in \u001b[0;36msave_subset_weights_to_hdf5_group\u001b[0;34m(f, weights)\u001b[0m\n",
      "\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_subset_weights_to_hdf5_group\u001b[39m(f, weights):\n",
      "\u001b[1;32m    717\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save top-level weights of a model to a HDF5 group.\u001b[39;00m\n",
      "\u001b[1;32m    718\u001b[0m \n",
      "\u001b[1;32m    719\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n",
      "\u001b[1;32m    720\u001b[0m \u001b[38;5;124;03m        f: HDF5 group.\u001b[39;00m\n",
      "\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m        weights: List of weight variables.\u001b[39;00m\n",
      "\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 723\u001b[0m     weight_values \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    724\u001b[0m     weight_names \u001b[38;5;241m=\u001b[39m [w\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights]\n",
      "\u001b[1;32m    725\u001b[0m     save_attributes_to_hdf5_group(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_names\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight_names)\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n",
      "\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n",
      "\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n",
      "\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/backend.py:4240\u001b[0m, in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n",
      "\u001b[1;32m   4228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the value of more than one tensor variable.\u001b[39;00m\n",
      "\u001b[1;32m   4229\u001b[0m \n",
      "\u001b[1;32m   4230\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   4237\u001b[0m \u001b[38;5;124;03m    RuntimeError: If this method is called inside defun.\u001b[39;00m\n",
      "\u001b[1;32m   4238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m   4239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n",
      "\u001b[0;32m-> 4240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [x\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tensors]\n",
      "\u001b[1;32m   4241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39minside_function():\n",
      "\u001b[1;32m   4242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get value inside Tensorflow graph function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/keras/backend.py:4240\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m   4228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the value of more than one tensor variable.\u001b[39;00m\n",
      "\u001b[1;32m   4229\u001b[0m \n",
      "\u001b[1;32m   4230\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   4237\u001b[0m \u001b[38;5;124;03m    RuntimeError: If this method is called inside defun.\u001b[39;00m\n",
      "\u001b[1;32m   4238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m   4239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n",
      "\u001b[0;32m-> 4240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tensors]\n",
      "\u001b[1;32m   4241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39minside_function():\n",
      "\u001b[1;32m   4242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get value inside Tensorflow graph function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:637\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    636\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n",
      "\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    638\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n",
      "\u001b[1;32m    639\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n",
      "\u001b[1;32m   1135\u001b[0m \n",
      "\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n",
      "\u001b[1;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n",
      "\u001b[0;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "\n",
      "File \u001b[0;32m~/github_repo/dagshub_p7/.conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[1;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
